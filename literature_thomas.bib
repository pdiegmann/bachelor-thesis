@article{Ackermann.2012,
 abstract = {Despite increasing interest in IT outsourcing (ITO) and the various benefits it promises, Cloud Computing (CC) as the currently most prevalent ITO paradigm still entails serious IT security risks. Little attention has been paid so far to fully and unambiguously capture the complex nature of IT security risks and how to measure it. Against this backdrop, we first propose a comprehensive conceptualization of Perceived IT Security Risks (PITSR) in the CC context that is based on six distinct risk dimensions grounded on an extensive literature review, Q-sorting, and expert interviews. Second, a multiple-indicators and multiple-causes analysis of data collected from 356 organizations is found to support the proposed conceptualization as a second-order aggregate construct. The results of our study contribute to IT security and ITO research, help (potential) adopters to assess risks, and enable CC providers to develop targeted strategies to mitigate risks perceived as crucial.},
 author = {Ackermann, Tobias and Widjaja, Thomas and Benlian, Alexander and Buxmann, Peter},
 year = {2012},
 title = {Perceived IT Security Risks of Cloud Computing: Conceptualization and Scale Development},
 pages = {1--20},
 pagination = {page},
 journal = {International Conference on Information Systems}
}


@article{Addis.2013,
 abstract = {Worldwide interest in the delivery of computing and storage capacity as a service continues to grow at a rapid pace. The complexities of such cloud computing centers require advanced resource management solutions that are capable of dynamically adapting the cloud platform while providing continuous service and performance guarantees. The goal of this paper is to devise resource allocation policies for virtualized cloud environments that satisfy performance and availability guarantees and minimize energy costs in very large cloud service centers. We present a scalable distributed hierarchical framework based on a mixed-integer non-linear optimization for resource management acting at multiple time-scales. Extensive experiments across a wide variety of configurations demonstrate the efficiency and effectiveness of our approach.},
 author = {Addis, Bernardetta and Ardagna, Danilo and Panicucci, Barbara and Squillante, Mark and Zhang, Li},
 year = {2013},
 title = {A Hierarchical Approach for the Resource Management of Very Large Cloud Platforms},
 pages = {1--30},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Dependable and Secure Computing}
}


@article{Ahmad.2011,
 abstract = {Public Cloud Computing (PCC) delivers technology {\dq}As a Service{\dq}. It is widely accepted by consumers, enterprises and, even governments because it reduces financial budget for acquiring Information technology (IT) infrastructure. The major deterrence against its adoption is security and governance risks. These risks are associated with three facets, geographical location of cloud provider, change of governance level within cloud service layers and inadequacy of existing international security standards to maintain security. In this paper, these three domains are researched to formulate governance life cycle framework for managing user data security in PCC.},
 author = {Ahmad, Rizwan and Janczewski, Lech},
 year = {2011},
 title = {Governance Life Cycle Framework for Managing Security in Public Cloud: From User Perspective},
 keywords = {Cloud Computing;governance;Management;security},
 pages = {372--379},
 pagination = {page},
 journal = {IEEE International Conference on Cloud Computing}
}


@misc{Badger.2011,
 author = {Badger, Lee and Grance, Tim and Patt-Corner, Robert and Voas, Jeff},
 year = {2011},
 title = {DRAFT Cloud Computing Synopsis and Recommendations: Recommendations of the National Institute of Standards and Technologie},
 url = {http://csrc.nist.gov/publications/drafts/800-146/Draft-NIST-SP800-146.pdf},
 urldate = {2013-03-27}
}


@article{Bernius.2012,
 abstract = {The adoption of Cloud Computing (CC) is growing rapidly. However, studies on the adoption of these new technologies by individual users are rare and almost exclusively focused on the business context. This paper presents first results of a research project addressing the adoption of CC by individual researchers and small research groups in Higher Education institutions. We surveyed users of the Frankfurt Cloud, an IaaS environment provided at the Goethe University of Frankfurt that serves affiliated researchers with on-demand computing and storage resources. On the one hand, the findings indicate that users benefit from fast and easy access to computing power for their research, on the other hand, user concerns related to cloud adoption are identified, which have to be taken into account during further development of CC services for academic research.},
 author = {Bernius, Steffen and Kr{\"o}nung, Julia},
 year = {2012},
 title = {Fostering Academic Research by Cloud Computing - The Users' Perspective},
 pages = {1--8},
 pagination = {page},
 journal = {European Conference on Information Systems}
}


@misc{Boss.2007,
 abstract = {This paper describes cloud computing, a computing platform for the next generation of the Internet. The paper defines clouds, explains the business benefits of cloud computing, and outlines cloud architecture and its major components. Readers will discover how a business can use cloud computing to foster innovation and reduce IT costs. IBM's implementation of cloud computing is described.},
 author = {Boss, Greg and Malladi, Padma and Quan, Dennis and Legregni, Linda and Hall, Harold},
 editor = {{High Performance On Demand Solutions (HiPODS)}},
 year = {2007},
 title = {Cloud Computing},
 url = {www.ibm.com/developerworks/websphere/zones/hipods/},
 urldate = {2013-03-09},
 number = {1369}
}


@article{Brandt.2012,
 abstract = {In recent years the emergence of Software as a Service (SaaS) provision and cloud computing in general had a tremendous impact on corporate information technology. While the implementation and successful operation of powerful information systems continues to be a cornerstone of success in modern enterprises, the ability to acquire IT infrastructure, software, or platforms on a pay-as-you-go basis has opened a new avenue for optimizing operational costs and processes. In this context we target elastic SaaS systems with on-demand cloud resource provisioning and implement an autonomic management artifact. Our framework forecasts future user behavior based on historic data, analyzes the impact of different workload levels on system performance based on a non-linear performance model, analyzes the economic impact of different provisioning strategies, derives an optimal operation strategy, and automatically assigns requests from users belonging to different Quality of Service (QoS) classes to the appropriate server instances. More generally, our artifact optimizes IT system operation based on a holistic evaluation of key aspects of service operation (e.g., system usage patterns, system performance, Service Level Agreements). The evaluation of our prototype, based on a real production system workload trace, indicates a cost-of-operation reduction by up to 60 percent without compromising QoS requirements.},
 author = {Brandt, Tobias and Tian, Ye and Hedwig, Markus and Neumann, Dirk},
 year = {2012},
 title = {Autonomic Management of Software as a Service Systems with Multiple Quality of Service Classes},
 pages = {1--13},
 pagination = {page},
 journal = {European Conference on Information Systems}
}


@article{Carey.2012,
 abstract = {Today's business practices require access to enterprise data by both external as well as internal applications. Suppliers expose part of their inventory to retailers, and health providers allow patients to view their health records. In turn, application developers sitting on the {\dq}wrong side{\dq} of the Web from their data need a mechanism to find out which data they can access, what their semantics are, and how they can integrate data from multiple enterprises. Data services are software components that address these issues by providing rich metadata, expressive languages, and APIs for service consumers to use to send queries and receive data from service providers. A data service can be employed on top of data stores providing different interfaces, such as database servers, CRM applications, or cloud-based storage systems, and using diverse underlying data models, such as relational, XML, or simple key/value pairs. A handful of emerging trends that can possibly direct future data services research and development are: 1. query formulation tools, 2. data service query optimization, 3. very large functional models, 4. cloud data service integration, 5. data summaries, and 6. cloud data service security.},
 author = {Carey, Michael J. and Onose, Nicola and Petropoulos, Michalis},
 year = {2012},
 title = {Data Services},
 pages = {86},
 pagination = {page},
 volume = {55},
 number = {6},
 journal = {Association for Computing Machinery. Communications of the ACM}
}


@article{Chadwick.2012,
 abstract = {In this paper we describe a policy based authorisation infrastructure that a cloud provider can run as an infrastructure service for its users. It will protect the privacy of usersʼ data by allowing the users to set their own privacy policies, and then enforcing them so that no unauthorised access is allowed to their data. The infrastructure ensures that the usersʼ privacy policies are stuck to their data, so that access will always be controlled by the policies even if the data is transferred between cloud providers or services. This infrastructure also ensures the enforcement of privacy policies which may be written in different policy languages by multiple authorities such as: legal, data subject, data issuer and data controller. A conflict resolution strategy is presented which resolves conflicts among the decisions returned by the different policy decision points (PDPs). The performance figures are presented which show that the system performs well and that each additional PDP only imposes a small overhead},
 author = {Chadwick, David W. and Fatema, Kaniz},
 year = {2012},
 title = {A Privacy Preserving Authorisation System for the Cloud},
 pages = {1359--1373},
 pagination = {page},
 volume = {78},
 number = {5},
 journal = {Journal of Computer {\&} System Sciences}
}


@article{Chaisiri.2012,
 abstract = {In cloud computing, cloud providers can offer cloud consumers two provisioning plans for computing resources, namely reservation and on-demand plans. In general, cost of utilizing computing resources provisioned by reservation plan is cheaper than that provisioned by on-demand plan, since cloud consumer has to pay to provider in advance. With the reservation plan, the consumer can reduce the total resource provisioning cost. However, the best advance reservation of resources is difficult to be achieved due to uncertainty of consumer's future demand and providers' resource prices. To address this problem, an optimal cloud resource provisioning (OCRP) algorithm is proposed by formulating a stochastic programming model. The OCRP algorithm can provision computing resources for being used in multiple provisioning stages as well as a long-term plan, e.g., four stages in a quarter plan and twelve stages in a yearly plan. The demand and price uncertainty is considered in OCRP. In this paper, different approaches to obtain the solution of the OCRP algorithm are considered including deterministic equivalent formulation, sample-average approximation, and Benders decomposition. Numerical studies are extensively performed in which the results clearly show that with the OCRP algorithm, cloud consumer can successfully minimize total cost of resource provisioning in cloud computing environments.},
 author = {Chaisiri, Sivadon and Lee, Bu-Sung and Niyato, Dusit},
 year = {2012},
 title = {Optimization of Resource Provisioning Cost in Cloud Computing},
 pages = {164--177},
 pagination = {page},
 volume = {5},
 number = {2},
 journal = {IEEE Transactions on Services Computing}
}


@article{Chang.2012a,
 abstract = {Video surveillance systems are playing an important role to protect lives and assets of individuals, enterprises and governments. Due to the prevalence of wired and wireless access to Internet, it would be a trend to integrate present isolated video surveillance systems by applying distributed computing environment and to further gestate diversified multimedia intelligent surveillance (MIS) applications in ubiquity. In this paper, we propose a distributed and secure architecture for ubiquitous video surveillance (UVS) services over Internet and error-prone wireless networks with scalability, ubiquity and privacy. As cloud computing, users consume UVS related resources as a service and do not need to own the physical infrastructure, platform, or software. To protect the service privacy, preserve the service scalability and provide reliable UVS video streaming for end users, we apply the AES security mechanism, multicast overlay network and forward error correction (FEC), respectively. Different value-added services can be created and added to this architecture without introducing much traffic load and degrading service quality. Besides, we construct an experimental test-bed for UVS system with three kinds of services to detect fire and fall-incident features and record the captured video at the same time. Experimental results showed that the proposed distributed service architecture is effective and numbers of services on different multicast islands were successfully connected without influencing the playback quality. The average sending rate and the receiving rates of these services are quite similar, and the surveillance video is smoothly played.},
 author = {Chang, Ray-I and Wang, Te-Chih and Wang, Chia-Hui and Liu, Jen-Chang and Ho, Jan-Ming},
 year = {2012},
 title = {Effective Distributed Service Architecture for Ubiquitous Video Surveillance},
 pages = {499--515},
 pagination = {page},
 volume = {14},
 number = {3},
 journal = {Information Systems Frontiers}
}


@article{Chang.2012b,
 abstract = {The market leaders of Cloud Computing try to leverage the parallel-processing capability of GPUs to provide more economic services than traditions. As the cornerstone of enterprise applications, database systems are of the highest priority to be improved for the performance and design complexity reduction. It is the purpose of this paper to design an in-memory database, called CUDADB, to scale up the performance of the database system on GPU with CUDA. The details of implementation and algorithms are presented, and the experiences of GPU-enabled CUDA database operations are also shared in this paper. For performance evaluation purposes, SQLite is used as the comparison target. From the experimental results, CUDADB performs better than SQLite for most test cases. And, surprisingly, the CUDADB performance is independent from the number of data records in a query result set. The CUDADB performance is a static proportion of the total number of data records in the target table. Finally, this paper comes out a concept of turning point that represents the difference ratio between CUDADB and SQLite.},
 author = {Chang, Yue-Shan and Sheu, Ruey-Kai and Yuan, Shyan-Ming and Hsu, Jyn-Jie},
 year = {2012},
 title = {Scaling Database Performance on GPUs},
 pages = {909--924},
 pagination = {page},
 volume = {14},
 number = {4},
 journal = {Information Systems Frontiers}
}


@article{Chard.2012,
 abstract = {Online relationships in social networks are often based on real world relationships and can therefore be used to infer a level of trust between users. We propose leveraging these relationships to form a dynamic {\dq}Social Cloud,'' thereby enabling users to share heterogeneous resources within the context of a social network. In addition, the inherent socially corrective mechanisms (incentives, disincentives) can be used to enable a cloud-based framework for long term sharing with lower privacy concerns and security overheads than are present in traditional cloud environments. Due to the unique nature of the Social Cloud, a social market place is proposed as a means of regulating sharing. The social market is novel, as it uses both social and economic protocols to facilitate trading. This paper defines Social Cloud computing, outlining various aspects of Social Clouds, and demonstrates the approach using a social storage cloud implementation in Facebook.},
 author = {Chard, Kyle and Bubendorfer, Kris and Caton, Simon and Rana, Omer F.},
 year = {2012},
 title = {Social Cloud Computing: A Vision for Socially Motivated Resource Sharing},
 pages = {551--563},
 pagination = {page},
 volume = {5},
 number = {4},
 journal = {IEEE Transactions on Services Computing}
}


@article{Chung.2013,
 abstract = {Cloud security is one of most important issues that has attracted a lot of research and development effort in past few years. Particularly, attackers can explore vulnerabilities of a cloud system and compromise virtual machines to deploy further large-scale Distributed Denial-of-Service (DDoS). DDoS attacks usually involve early stage actions such as multi-step exploitation, low frequency vulnerability scanning, and compromising identified vulnerable virtual machines as zombies, and finally DDoS attacks through the compromised zombies. Within the cloud system, especially the Infrastructure-as-a-Service (IaaS) clouds, the detection of zombie exploration attacks is extremely difficult. This is because cloud users may install vulnerable applications on their virtual machines. To prevent vulnerable virtual machines from being compromised in the cloud, we propose a multi-phase distributed vulnerability detection, measurement, and countermeasure selection mechanism called NICE, which is built on attack graph based analytical models and reconfigurable virtual network-based countermeasures. The proposed framework leverages OpenFlow network programming APIs to build a monitor and control plane over distributed programmable virtual switches in order to significantly improve attack detection and mitigate attack consequences. The system and security evaluations demonstrate the efficiency and effectiveness of the proposed solution.},
 author = {Chung, Chen-Jen and Khatkar, Pankaj and Xing, Tianyi and Lee, Jeongkeun and Huang, Dijiang},
 year = {2013},
 title = {NICE: Network Intrusion Detection and Countermeasure Selection in Virtual Network Systems},
 pages = {1--14},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Dependable and Secure Computing}
}


@article{Conboy.2012,
 abstract = {This study goes beyond a binary view of adoption and instead examines the more complex nature of cloud computing assimilation. It examines (i) acceptance, the extent to which an organisation's members are committed to the adoption, (ii) routinisation, the extent to which the use of cloud becomes a normal activity, and (iii) infusion, the extent to which more features of the cloud are used than originally planned, the extent to which use is sophisticated, and the extent to which use is emergent. Drawing on 3 case studies, this study will contribute to the existing cloud technologies literature that does not address the complex and multi-faceted nature of adoption. Secondly, it will provide an insight into cloud computing adoption by focusing on the benefits and challenges associated with implementation in organisations. Thirdly, we also seek to develop a set of recommended practices for overcoming such barriers to adoption.},
 author = {Conboy, Kieran and Morgan, Lorraine},
 year = {2012},
 title = {Assimilation of the Cloud: Challenges to Acceptance, Routinisation and Infusion of Cloud Computing},
 pages = {1--8},
 pagination = {page},
 journal = {International Conference on Information Systems}
}


@article{Cullen.2005,
 abstract = {Information technology outsourcing (ITO) and, more recently, business process outsourcing (BPO) have received considerable practitioner and academic attention. But no study has yet presented a rigorously developed, structured, and detailed process for client organizations to follow to improve their likelihood of success while minimizing their risk. Based on 100 outsourcing cases from 1994 to 2003, we present a four-phase Outsourcing Life Cycle Model with 54 key activities. The model was tested in 2004 via in-depth interviews in seven large organizations. Our major finding is that the more of this process an outsourcing organization conducts, and conducts well, the greater its success, regardless of its outsourcing objectives.},
 author = {Cullen, Sara and Seddon, Peter and Willcocks, Leslie},
 year = {2005},
 title = {Managing Outsourcing: The Life Cycle Imperative},
 pages = {229--246},
 pagination = {page},
 volume = {4},
 number = {1},
 journal = {MIS Quarterly Executive}
}


@article{Curbera.2002,
 abstract = {This tutorial explores the most salient and stable specifications in each of the three major areas of the emerging Web services framework. They are the simple object access protocol, the Web Services Description Language and the Universal Description, Discovery, and Integration directory, which is a registry of Web services descriptions.},
 author = {Curbera, Francisco and Duftler, Matthew J. and Khalaf, Rania and Nagy, William A. and Mukhi, Nirmal K. and Weerawarana, Sanjiva},
 year = {2002},
 title = {Unraveling the Web Services Web: An Introduction to SOAP, WSDL, and UDDI},
 keywords = {Access protocols;Encoding;Packaging;Pricing;Quality of Service, Simple object access protocol, TRAVEL SERVICES;web and internet services, web services, xml},
 pages = {86--93},
 pagination = {page},
 volume = {6},
 number = {2},
 issn = {1089-7801},
 journal = {IEEE Internet Computing},
 doi = {10.1109/4236.991449}
}


@article{Doulamis.2012,
 abstract = {Resource selection and task assignment are basic operations in distributed computing environments, like the Grid and the Cloud, where tasks compete for resources. The decisions made by the corresponding algorithms should be judged based not only on metrics related to user satisfaction but also based on resource-related performance metrics. In our work we focus on the case of tasks with fixed but not strict time requirements, given in the form of a requested start and finish time. We propose an algorithm for assigning tasks to resources that minimizes the violations of the tasks' time requirements while simultaneously maximizing the resources' utilization efficiency for a given number of resources. The exact time scheduling of the tasks on the resources is then decided by taking into account the time constraints. The proposed scheme exploits concepts derived from graph partitioning, and groups together tasks so as to a) minimize the time overlapping of the tasks assigned to a given resource and b) maximize the time overlapping among tasks assigned to different resources. The partitioning is performed using a spectral clustering methodology through normalized cuts. Experimental results show that the proposed algorithm outperforms other scheduling algorithms.},
 author = {Doulamis, Nikolaos and Kokkinos, Panagiotis and Varvarigos, Emmanouel},
 year = {2012},
 title = {Resource Selection for Tasks with Time Requirements using Spectral Clustering},
 pages = {1--14},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Computers}
}


@article{ElKihal.2012,
 abstract = {Today's pricing of infrastructure-as-a-service is not transparent because some providers, such as Google, charge separately for each service characteristic (e.g., $50 per CPU or $15 per GB of memory per month) and let customers freely configure the service. In contrast, competitors like Amazon, Microsoft, and IBM only offer predefined bundles (e.g., 4 GB of memory, 400 GB of storage, and 2 CPUs for $140 per month). These different types of pricing plans make price comparisons very difficult. The aim of this study is to increase price transparency among providers by proposing two price comparison methods, which provide a detailed overview of the billing situation in the infrastructure-as-a-service market. The first method is {\dq}hedonic pricing{\dq}, which decomposes each provider's billing into the contributing values of the product´s characteristics. The second is a new method, called PriCo ({\dq}Pricing plan Comparison{\dq}), which in addition considers offers from competitive providers. We employ the two methods in an empirical study, which compares the pricing of the infrastructure-as-a-service providers Google, Microsoft, Amazon, IBM, and Terremark. The insights gained allow customers to better identify the best provider for their needs. The proposed methods also help providers to better understand and position their pricing in the market.},
 author = {El Kihal, Siham and Schlereth, Christian and Skiera, Bernd},
 year = {2012},
 title = {Price Comparison for Infrastructure-as-a-Service},
 pages = {1--12},
 pagination = {page},
 journal = {European Conference on Information Systems}
}


@article{Esteves.2007,
 author = {Esteves, Jose and Bohorquez, Victor},
 year = {2007},
 title = {An Updated ERP Systems Annotated Bibliography: 2001-2005},
 pages = {386--446},
 pagination = {page},
 volume = {19},
 number = {18},
 journal = {Communications of the Association for Information Systems}
}


@article{Garrison.2012,
 abstract = {Cloud computing is a model for enabling convenient, on-demand network access to a shared pool of configurable computing resources that can be quickly provisioned and released with minimal management effort or service provider interaction. The emergence of cloud computing is transforming the way organizations purchase and manage computing resources, providing a fundamentally different IT model in which a cloud provider might be responsible for a range of IT activities, including hardware and software installation, upgrades, maintenance, backup, data storage, and security. Successful deployment denotes the realization of unique or valuable organizational benefits that are a source of differentiation and competitive advantage. The authors found that trust, managerial capability, and technical capability each have a significant relationship with cloud-deployment performance. The results of the user-vendor partnership imply that when a client organization and its cloud vendor develop a relationship characterized by trust, the client is more likely to realize the technical and economic benefits for which it originally pursued cloud computing.},
 author = {Garrison, Gary and Kim, Sanghyun and Wakefield, Robin L.},
 year = {2012},
 title = {Success Factors for Deploying Cloud Computing},
 pages = {62--68},
 pagination = {page},
 volume = {55},
 number = {9},
 journal = {Association for Computing Machinery. Communications of the ACM}
}


@article{Giessmann.2012,
 abstract = {Platform as a Service (PaaS) solutions are changing the way that software is produced, distributed, consumed, and priced. PaaS, also known as cloud platform, offer an execution environment based on software platforms. To be competitive on the market, PaaS providers have to be aware of drivers of successful platforms and design or adjust their business models accordingly. Surprisingly, prior research has made little attempt to investigate consumers' preferences on PaaS that influence developers' choice on PaaS solutions. This paper examines this understudied issue through a conjoint study. First a comprehensive literature analysis on PaaS has been conducted in order to build the study design on a rigorous foundation. The conducted conjoint survey contained ten attributes together with 26 corresponding attribute levels and has been completed by 103 participants. Based on the results, a prioritized list of customers' preferences for PaaS has been created.},
 author = {Giessmann, Andrea and Stanoevska, Katarina},
 year = {2012},
 title = {Platform as a Service - A Conjoint Study on Consumers' Preferences},
 pages = {1--20},
 pagination = {page},
 journal = {International Conference on Information Systems}
}


@article{Goiri.2012,
 abstract = {Resource provisioning in Cloud providers is a challenge because of the high variability of load over time. On the one hand, the providers can serve most of the requests owning only a restricted amount of resources, but this forces to reject customers during peak hours. On the other hand, valley hours incur in under-utilization of the resources, which forces the providers to increase their prices to be profitable. Federation overcomes these limitations and allows providers to dynamically outsource resources to others in response to demand variations. Furthermore, it allows providers with underused resources to rent them to other providers. Both techniques make the provider getting more profit when used adequately. Federation of Cloud providers requires having a clear understanding of the consequences of each decision. In this paper, we present a characterization of providers operating in a federated Cloud which helps to choose the most convenient decision depending on the environment conditions. These include when to outsource to other providers, rent free resources to other providers (i.e., insourcing), or turn off unused nodes to save power. We characterize these decisions as a function of several parameters and implement a federated provider that uses this characterization to exploit federation. Finally, we evaluate the profitability of using these techniques using the data from a real provider},
 author = {Goiri, {\'I}{\~n}igo and Guitart, Jordi and Torres, Jordi},
 year = {2012},
 title = {Economic Model of a Cloud Provider Operating in a Federated Cloud},
 pages = {827--843},
 pagination = {page},
 volume = {14},
 number = {4},
 journal = {Information Systems Frontiers}
}


@article{GutierrezGarcia.2012,
 abstract = {Executing bag-of-tasks applications in multiple Cloud environments while satisfying both consumers' budgets and deadlines poses the following challenges: How many resources and how many hours should be allocated? What types of resources are required? How to coordinate the distributed execution of bag-of-tasks applications in resources composed from multiple Cloud providers?. This work proposes a genetic algorithm for estimating suboptimal sets of resources and an agent-based approach for executing bag-of-tasks applications simultaneously constrained by budgets and deadlines. Agents (endowed with distributed algorithms) compose resources and coordinate the execution of bag-of-tasks applications. Empirical results demonstrate that the genetic algorithm can autonomously estimate sets of resources to execute budget-constrained and deadline-constrained bag-of-tasks applications composed of more economical (but slower) resources in the presence of loose deadlines, and more powerful (but more expensive) resources in the presence of large budgets. Furthermore, agents can efficiently and successfully execute randomly generated bag-of-tasks applications in multi-Cloud environments.},
 author = {Gutierrez-Garcia, J. Octavio and Sim, Kwang Mong},
 year = {2012},
 title = {GA-Based Cloud Resource Estimation for Agent-Based Execution of Bag-of-Tasks Applications},
 pages = {925--951},
 pagination = {page},
 volume = {14},
 number = {4},
 journal = {Information Systems Frontiers}
}


@article{He.2012,
 abstract = {Distributing multiple replicas in geographically-dispersed clouds is a popular approach to reduce latency to users. It is important to ensure that each replica should have availability and data integrity features; that is, the same as the original data without any corruption and tampering. Remote data possession checking is a valid method to verify the replicasʼs availability and integrity. Since remotely checking the entire data is time-consuming due to both the large data volume and the limited bandwidth, efficient data-possession-verifying methods generally sample and check a small hash (or random blocks) of the data to greatly reduce the I/O cost. Most recent research on data possession checking considers only single replica. However, multiple replicas data possession checking is much more challenging, since it is difficult to optimize the remote communication cost among multiple geographically-dispersed clouds. In this paper, we provide a novel efficient Distributed Multiple Replicas Data Possession Checking (DMRDPC) scheme to tackle new challenges. Our goal is to improve efficiency by finding an optimal spanning tree to define the partial order of scheduling multiple replicas data possession checking. But since the bandwidths have geographical diversity on the different replica links and the bandwidths between two replicas are asymmetric, we must resolve the problem of Finding an Optimal Spanning Tree in a Complete Bidirectional Directed Graph, which we call the FOSTCBDG problem. Particularly, we provide theories for resolving the FOSTCBDG problem through counting all the available paths that viruses attack in clouds network environment. Also, we help the cloud users to achieve efficient multiple replicas data possession checking by an approximate algorithm for tackling the FOSTCBDG problem, and the effectiveness is demonstrated by an experimental study.},
 author = {He, Jing and Zhang, Yanchun and Huang, Guangyan and Shi, Yong and Cao, Jie},
 year = {2012},
 title = {Distributed Data Possession Checking for Securing Multiple Replicas in Geographically-Dispersed Clouds},
 pages = {1345--1358},
 pagination = {page},
 volume = {78},
 number = {5},
 journal = {Journal of Computer {\&} System Sciences}
}


@article{Hedwig.2012,
 abstract = {The growing awareness of the substantial environmental footprint of Information System has increasingly focused corporate transformation efforts on the efficient usage of Information Technology. In this context, we provide a new concept to enterprise IS operation and introduce a novel adaptation framework that harmonizes operational requirements with efficiency goals. We concretely target elastic n-tier applications with dynamic on-demand resource provisioning for component servers and implement an adaptation engine prototype. Our framework forecasts future user behavior, analyzes the impact of workload on system performance, evaluates the economic impact of different provisioning strategies, and derives an optimal operation strategy. More generally, our adaptation engine optimizes IT system operation based on a holistic evaluation of the key factors of influence. In the evaluation, we systematically investigate practicability, optimization potential, as well as effectiveness. Additionally, we show that our framework allows flexible IS operation with up to a 40 percent lower cost of operation.},
 author = {Hedwig, Markus and Malkowski, Simon and Neumann, Dirk},
 year = {2012},
 title = {Efficient and Flexible Management of Enterprise Information Systems},
 pages = {1--17},
 pagination = {page},
 journal = {International Conference on Information Systems}
}


@article{Jin.2008,
 author = {Jin, Kevin and Ray, Pradeep K.},
 year = {2008},
 title = {Business-Oriented Development Methodology for IT Service Management},
 pages = {99--108},
 pagination = {page},
 journal = {41st Hawaii International Conference on System Sciences}
}


@article{Joshi.2012,
 abstract = {Managing virtualized services efficiently over the cloud is an open challenge. Traditional models of software development are not appropriate for the cloud computing domain, where software (and other) services are acquired on demand. In this paper, we describe a new integrated methodology for the lifecycle of IT services delivered on the cloud, and demonstrate how it can be used to represent and reason about services and service requirements and so automate service acquisition and consumption from the cloud. We have divided the IT service lifecycle into five phases of requirements, discovery, negotiation, composition, and consumption. We detail each phase and describe the ontologies that we have developed to represent the concepts and relationships for each phase. To show how this lifecycle can automate the usage of cloud services, we describe a cloud storage prototype that we have developed. This methodology complements previous work on ontologies for service descriptions in that it is focused on supporting negotiation for the particulars of a service and going beyond simple matchmaking.},
 author = {Joshi, Karuna P. and Yesha, Yelena and Finin, Tim},
 year = {2012},
 title = {Automating Cloud Services Lifecycle through Semantic Technologies},
 pages = {1--14},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Services Computing}
}


@article{Kangarlou.2012,
 abstract = {A virtual networked infrastructure (VNI) consists of virtual machines (VMs) connected by a virtual network. Created for individual users on a shared cloud infrastructure, VNIs reflect the concept of {\dq}Infrastructure as a Service'' (IaaS) as part of the emerging cloud computing paradigm. The ability to take snapshots of an entire VNI-including images of the VMs with their execution, communication, and storage states-yields a unique approach to reliability as a VNI snapshot can be used to restore the operation of the entire virtual infrastructure. We present VNsnap, a system that takes distributed snapshots of VNIs. Unlike many existing distributed snapshot/checkpointing solutions, VNsnap does not require any modifications to the applications, libraries, or (guest) operating systems (OSs) running in the VMs. Furthermore, by performing much of the snapshot operation concurrently with the VNI's normal operation, VNsnap incurs only seconds of downtime. We have implemented VNsnap on top of Xen. Our experiments with real-world parallel and distributed applications demonstrate VNsnap's effectiveness and efficiency.},
 author = {Kangarlou, Ardalan and Eugster, Patrick and Xu, Dongyan},
 year = {2012},
 title = {VNsnap: Taking Snapshots of Virtual Networked Infrastructures in the Cloud},
 pages = {484--496},
 pagination = {page},
 volume = {5},
 number = {4},
 journal = {IEEE Transactions on Services Computing}
}


@article{Kecskemeti.2013,
 abstract = {Infrastructure as a Service systems use virtual appliances to initiate virtual machines. As virtual appliances encapsulate applications and services with their support environment, their delivery is the most expensive task of the virtual machine creation. Virtual appliance delivery is a well-discussed topic in the field of cloud computing. However, for high efficiency, current techniques require the modification of the underlying IaaS systems. To target the wider adoptability of these delivery solutions, this article proposes the concept of minimal manageable virtual appliances (MMVA) that are capable of updating and configuring their virtual machines without the need to modify IaaS systems. To create MMVAs, we propose to reduce manageable virtual appliances until they become MMVAs. This research also reveals a methodology for appliance developers to incorporate MMVAs in their own appliances to enable their efficient delivery and wider adoptability. Finally, the article evaluates the positive effects of MMVAs on an already existing delivery solution: the Automated Virtual appliance creation Service (AVS). Through experimental evaluation, we present that the application of MMVAs not only increases the adoptability of a delivery solution but it also significantly improves its performance in highly-dynamic systems.},
 author = {Kecskemeti, Gabor and Terstyanszky, Gabor and Kacsuk, Peter and Nemeth, Zsolt},
 year = {2013},
 title = {Towards Efficient Virtual Appliance Delivery with Minimal Manageable Virtual Appliances},
 pages = {1--14},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Services Computing}
}


@article{Klein.2013,
 abstract = {Service-Oriented Computing enables the composition of loosely coupled services provided with varying Quality of Service (QoS) levels. Selecting a near-optimal set of services for a composition in terms of QoS is crucial when many functionally equivalent services are available. As the number of distributed services, especially in the cloud, is rising rapidly, the impact of the network on the QoS keeps increasing. Despite this, current approaches do not differentiate between the QoS of services themselves and the network. Therefore, the computed latency differs from the actual latency, resulting in suboptimal QoS. Thus, we propose a network-aware approach that handles the QoS of services and the QoS of the network independently. First, we build a network model in order to estimate the network latency between arbitrary services and potential users. Our selection algorithm then leverages this model to find compositions with a low latency for a given execution policy. We employ a self-adaptive genetic algorithm which balances the optimization of latency and other QoS as needed and improves the convergence speed. In our evaluation, we show that our approach works under realistic network conditions, efficiently computing compositions with much lower latency and otherwise equivalent QoS compared to current approaches.},
 author = {Klein, Adrian and Fuyuki, Ishikawa and Honiden, Shinichi},
 year = {2013},
 title = {SanGA: A Self-Adaptive Network-Aware Approach to Service Composition},
 pages = {1--14},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Services Computing}
}


@article{Knapper.2012,
 abstract = {The fundamental paradigm shift from a product- to a service-oriented economy implies novel technical and organizational challenges. The resulting dynamic of the technical infrastructure and the increasing development towards requesting external business services to be integrated into end-to-end business processes requires mechanisms ensuring the reliability of the organization?s composed services, workflows and business processes. From a business perspective, QoS characteristics defined based on technical services within the infrastructural layer have to be aggregated to more business-relevant Key Performance Indicators on business process layer to express the Quality of Process. These KPIs represent quality that is highly related to the business?s performance (e.g. processing time of a business service) and are crucial for achieving predefined goals in order to stay competitive in the market. The contribution of this paper is threefold: We (i) provide an in-depth requirements analysis for such a holistic quality management framework, we (ii) develop a holistic aggregation framework which enables service level aggregation incorporating the loosely coupled structure of business processes with invoked systems and services in an instance based manner. To demonstrate the expressive power of our framework we (iii) provide an exemplary industrial application scenario and illustrate the functioning and interplay of the designed artifacts.},
 author = {Knapper, Rico and Poodratchi, Daniel and Job, Lennart},
 year = {2012},
 title = {Quality of Process? A Business Process Perspective on Quality of Service},
 pages = {1--13},
 pagination = {page},
 journal = {European Conference on Information Systems}
}


@article{Kousiouris.2012,
 abstract = {In modern utility computing infrastructures, like Grids and Clouds, one of the significant actions of a Service Provider is to predict the resources needed by the services included in its platform in an automated fashion for service provisioning optimization. Furthermore, a variety of software toolkits exist that implement an extended set of algorithms applicable to workload forecasting. However, their automated use as services in the distributed computing paradigm includes a number of design and implementation challenges. In this paper, a decoupled framework is presented, for taking advantage of software like GNU Octave in the process of creating and using prediction models during the service lifecycle of a SOI. A performance analysis of the framework is also conducted. In this context, a methodology for creating parametric or gearbox services with multiple modes of operations based on the execution conditions is portrayed and is applied to transform the aforementioned service framework in order to optimize service performance. A new estimation algorithm is introduced, that creates performance rules of applications as black boxes, through the creation and usage of genetically optimized artificial neural networks. Through this combination, the critical parameters of the networks are decided through an evolutionary iterative process.},
 author = {Kousiouris, George and Menychtas, Andreas and Kyriazis, Dimosthenis and Konstanteli, Kleopatra and Gogouvitis, Spyridon and Katsaros, Gregory and Varvarigou, Theodora},
 year = {2012},
 title = {Parametric Design and Performance Analysis of a Decoupled Service-Oriented Prediction Framework based on Embedded Numerical Software},
 pages = {1--13},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Services Computing}
}


@article{Krishnan.2012,
 abstract = {Whether it is a hurricane blowing down power lines, a volcanic-ash cloud grounding all flights for a continent, or a humble rodent gnawing through underground fibers -- the unexpected happens. You cannot do much to prevent it, but there is a lot you can do to be prepared for it. To this end, Google runs an annual, companywide, multi-day Disaster Recovery Testing event -- DiRT -- the objective of which is to ensure that Google's services and internal business operations continue to run following a disaster. A simple example is testing an organization's ability to recover from the loss of a data center. Such a loss may be simulated by powering down the facility or by causing network links to fail. DiRT tests can be disruptive, and failures should be expected to occur at any point. Several steps can be taken to minimize potential damage.},
 author = {Krishnan, Kripa},
 year = {2012},
 title = {Weathering the Unexpected},
 pages = {48},
 pagination = {page},
 volume = {55},
 number = {11},
 journal = {Communications of the ACM}
}


@book{Laudon.2010,
 author = {Laudon, Kenneth C. and Laudon, Jane P. and Schoder, Detlef},
 year = {2010},
 title = {Wirtschaftsinformatik: Eine Einf{\"u}hrung},
 keywords = {Wirtschaftsinformatik;Betriebliches Informationssystem;Informationsmanagement;Theorie;Wirtschaftsinformatik - Lehrbuch},
 address = {M{\"u}nchen and Boston and San Francisco and Harlow and England and Don Mills and Ontario and Sydney and Mexico City and Madrid and Amsterdam},
 edition = {2},
 publisher = {Pearson Studium},
 isbn = {3827373484}
}


@article{Li.2012,
 abstract = {Virtualization is a rapidly evolving technology that can be used to provide a range of benefits to computing systems, including improved resource utilization, software portability, and reliability. Virtualization also has the potential to enhance security by providing isolated execution environments for different applications that require different levels of security. For security-critical applications, it is highly desirable to have a small trusted computing base (TCB), since it minimizes the surface of attacks that could jeopardize the security of the entire system. In traditional virtualization architectures, the TCB for an application includes not only the hardware and the virtual machine monitor (VMM), but also the whole management operating system (OS) that contains the device drivers and virtual machine (VM) management functionality. For many applications, it is not acceptable to trust this management OS, due to its large code base and abundance of vulnerabilities. For example, consider the {\dq}computing-as-a-service'' scenario where remote users execute a guest OS and applications inside a VM on a remote computing platform. It would be preferable for many users to utilize such a computing service without being forced to trust the management OS on the remote platform. In this paper, we address the problem of providing a secure execution environment on a virtualized computing platform under the assumption of an untrusted management OS. We propose a secure virtualization architecture that provides a secure runtime environment, network interface, and secondary storage for a guest VM. The proposed architecture significantly reduces the TCB of security-critical guest VMs, leading to improved security in an untrusted management environment. We have implemented a prototype of the proposed approach using the Xen virtualization system, and demonstrated how it can be used to facilitate secure remote computing services. We evaluate the performance penalties incurre- by the proposed architecture, and demonstrate that the penalties are minimal.},
 author = {Li, Chunxiao and Raghunathan, Anand and Jha, Niraji K.},
 year = {2012},
 title = {A Trusted Virtual Machine in an Untrusted Management Environment},
 pages = {472--483},
 pagination = {page},
 volume = {5},
 number = {4},
 journal = {IEEE Transactions on Services Computing}
}


@article{Lindner.2011,
 abstract = {Cloud Computing is offering competitive advantages
to companies through ﬂexible and, scalable access to computing
resources. More and more companies are moving to cloud
environments; therefore understanding the requirements for this
process is both important and beneﬁcial. The requirements for
migrating from a traditional computing environment to a cloud
hosting environment are discussed in this paper, considering this
migration from a supply chain lifecycle perspective. The cloud
supply chain is examined from a lifecycle perspective for the
management of the migration project. This paper illustrates the
requirements that need to be considered when adopting a cloud
migration strategy and the steps to take in order to manage this
process.},
 author = {Lindner, Maik A. and McDonald, Fiona and Conway, Gerard and Curry, Edward},
 year = {2011},
 title = {Understanding Cloud Requirements - A Supply Chain Lifecycle Approach},
 keywords = {Cloud Computing;supply chain;cloud sourcing;cloud lifecycle},
 pages = {20},
 pagination = {page},
 number = {20-25},
 journal = {The Second International Conference on Cloud Computing, GRIDs, and Virtualization}
}


@article{Liu.2012,
 abstract = {New applications based on cloud computing, such as data synchronization for large chain departmental stores and bank transaction records, require very high-speed data transport. Although a number of high-bandwidth networks have been built, existing transport protocols or their variants over such networks cannot fully exploit the network bandwidth. Our experiments show that the fixed-size application level buffer employed in the receiver side is a major cause of this deficiency. A buffer that is either too small or too large impairs the transfer performance. In this paper, we propose Rada, a dynamic receiving buffer adaptation scheme for high-speed data transfer. Rada employs an Exponential Moving Average aided scheme to quantify the data arrival rate and consumption rate in the buffer. Based on these two rates, we develop a Lineal Aggressive Increase Conservative Decrease scheme to adjust the buffer size dynamically. Moreover, a Weighted Mean Function is employed to make the adjustment adaptive to the available memory in the receiver. The performance of Rada is theoretically compared with potential alternatives. Experimental results conform to the theoretical results, and show that Rada outperforms the static buffer scheme in terms of throughput, memory footprint, and fairness.},
 author = {Liu, Hao and Zhang, Yaoxue and Zhou, Yuezhi and Fu, Xiaoming and Yang, Laurence T.},
 year = {2012},
 title = {Receiving Buffer Adaptation for High-Speed Data Transfer},
 pages = {1--14},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Computers}
}


@article{Marozzo.2012,
 abstract = {MapReduce is a programming model for parallel data processing widely used in Cloud computing environments. Current MapReduce implementations are based on centralized master-slave architectures that do not cope well with dynamic Cloud infrastructures, like a Cloud of clouds, in which nodes may join and leave the network at high rates. We have designed an adaptive MapReduce framework, called P2P-MapReduce, which exploits a peer-to-peer model to manage node churn, master failures, and job recovery in a decentralized but effective way, so as to provide a more reliable MapReduce middleware that can be effectively exploited in dynamic Cloud infrastructures. This paper describes the P2P-MapReduce system providing a detailed description of its basic mechanisms, a prototype implementation, and an extensive performance evaluation in different network scenarios. The performance results confirm the good fault tolerance level provided by the P2P-MapReduce framework compared to a centralized implementation of MapReduce, as well as its limited impact in terms of network overhead.},
 author = {Marozzo, Fabrizio and Talia, Domenico and Trunfio, Paolo},
 year = {2012},
 title = {P2P-MapReduce: Parallel Data Processing in Dynamic Cloud Environments},
 pages = {1382--1402},
 pagination = {page},
 volume = {78},
 number = {5},
 journal = {Journal of Computer {\&} System Sciences}
}


@article{Marston.2011,
 abstract = {The evolution of cloud computing over the past few years is potentially one of the major advances in the history of computing. However, if cloud computing is to achieve its potential, there needs to be a clear understanding of the various issues involved, both from the perspectives of the providers and the consumers of the technology. While a lot of research is currently taking place in the technology itself, there is an equally urgent need for understanding the business-related issues surrounding cloud computing. In this article, we identify the strengths, weaknesses, opportunities and threats for the cloud computing industry. We then identify the various issues that will affect the different stakeholders of cloud computing. We also issue a set of recommendations for the practitioners who will provide and manage this technology. For IS researchers, we outline the different areas of research that need attention so that we are in a position to advice the industry in the years to come. Finally, we outline some of the key issues facing governmental agencies who, due to the unique nature of the technology, will have to become intimately involved in the regulation of cloud computing. [Copyright {\&}y{\&} Elsevier]},
 author = {Marston, Sean and Li, Zhi and Bandyopadhyay, Subhajyoti and Zhang, Juheng and Ghalsasi, Anand},
 year = {2011},
 title = {Cloud Computing - The Business Perspective},
 url = {10.1016/j.dss.2010.12.006},
 keywords = {Cloud Computing;Virtualisation;Software as a Service;SaaS;Platform as a Service;PaaS;Infrastructure as a Service;IaaS;On-demand computing;Cloud computing regulation},
 pages = {176--189},
 pagination = {page},
 volume = {51},
 number = {1},
 journal = {Decision Support Systems}
}


@article{Martens.2012,
 abstract = {In this article a sophisticated formal mathematical decision model is developed that supports the selection of Cloud Computing services in a multisourcing scenario. The objective is to determine the selection of appropriate Cloud Computing services offered by different providers. In order to do so, we consider cost as well as risk factors which are relevant to the decision scope. For example, coordination costs, IT service costs, maintenance costs and the costs of taken risks were compared. Risks are modeled by means of the three common security objectives integrity, confidentiality and availability. The managerial implications of the model lie in the sustainable decision support and the comprehensive decision approach. The formal model is prototypically implemented using a software tool and examined with the help of a simulation study in three realistic scenarios and a sensitivity analysis.},
 author = {Martens, Benedikt and Teuteberg, Frank},
 year = {2012},
 title = {Decision-Making in Cloud Computing Environments: A Cost and Risk based Approach},
 pages = {871--893},
 pagination = {page},
 volume = {13},
 number = {4},
 journal = {Information Systems Frontiers}
}


@article{Mazhelis.2012,
 abstract = {Adoption of cloud infrastructure promises enterprises numerous benefits, such as faster time-to-market and improved scalability enabled by on-demand provisioning of pooled and shared computing resources. In particular, hybrid clouds, by combining the private in-house capacity with the on-demand capacity of public clouds, promise to achieve both increased utilization rate of the in-house infrastructure and limited use of the more expensive public cloud, thereby lowering the total costs for a cloud user organization. In this paper, an analytical model of hybrid cloud costs is introduced, wherein the costs of computing and data communication are taken into account. Using this model, a cost-efficient division of the computing capacity between the private and the public portion of a hybrid cloud can be identified. By analyzing the model, it can be shown that, given fixed prices for private and public capacity, a hybrid cloud incurs the minimum costs. Furthermore, it is shown that, as the volume of data transferred to/from the public cloud increases, a greater portion of the capacity should be allocated to the private cloud. Finally, the paper illustrates analytically that, when the unit price of capacity declines with the volume of acquired capacity, a hybrid cloud may become more expensive than a private or a public cloud.},
 author = {Mazhelis, Oleksiy and Tyrv{\"a}inen, Pasi},
 year = {2012},
 title = {Economic Aspects of Hybrid Cloud Infrastructure: User Organization Perspective},
 pages = {845--869},
 pagination = {page},
 volume = {14},
 number = {4},
 journal = {Information Systems Frontiers}
}


@article{Mei.2013,
 abstract = {Server consolidation and application consolidation through virtualization are key performance optimizations in cloud-based service delivery industry. In this paper, we argue that it is important for both cloud consumers and cloud providers to understand the various factors that may have significant impact on the performance of applications running in a virtualized cloud. This paper presents an extensive performance study of network I/O workloads in a virtualized cloud environment. We first show that current implementation of virtual machine monitor (VMM) does not provide sufficient performance isolation to guarantee the effectiveness of resource sharing across multiple virtual machine instances (VMs) running on a single physical host machine, especially when applications running on neighboring VMs are competing for computing and communication resources. Then we study a set of representative workloads in cloud-based data centers, which compete for either CPU or network I/O resources, and present the detailed analysis on different factors that can impact the throughput performance and resource sharing effectiveness. For example, we analyze the cost and the benefit of running idle VM instances on a physical host where some applications are hosted concurrently. We also present an in-depth discussion on the performance impact of colocating applications that compete for either CPU or network I/O resources. Finally, we analyze the impact of different CPU resource scheduling strategies and different workload rates on the performance of applications running on different VMs hosted by the same physical machine.},
 author = {Mei, Yiduo and Liu, Ling and Pu, Xing and Sivathanu, Sankaran and Dong, Xiaoshe},
 year = {2013},
 title = {Performance Analysis of Network I/O Workloads in Virtualized Data Centers},
 pages = {48--63},
 pagination = {page},
 volume = {6},
 number = {1},
 journal = {IEEE Transactions on Services Computing}
}


@misc{Mell.2011,
 abstract = {Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. This cloud model is composed of five essential characteristics, three service models, and four deployment models},
 author = {Mell, Peter and Grance, Timothy},
 year = {2011},
 title = {The NIST Definition of Cloud Computing: Recommendations of the National Institute of Standards and Technologie},
 url = {http://csrc.nist.gov/publications/nistpubs/800-145/SP800-145.pdf},
 keywords = {Cloud Computing;Definition},
 urldate = {2013-03-04},
 number = {800-145}
}


@article{Meng.2012,
 abstract = {This paper introduces the concept of monitoring-as-a-service (MaaS), its main components, and a suite of key functional requirements of MaaS in cloud, including instantaneous and window-based state monitoring, periodic and violation-likelihood based monitoring, single tenant and multi-tenant state monitoring. We argue that MaaS should support not only the conventional state monitoring capabilities, such as instantaneous violation detection, periodical state monitoring and single tenant monitoring, but also performance-enhanced functionalities that can optimize on monitoring cost, scalability, and the effectiveness of monitoring service consolidation and isolation. In this paper we present three enhanced MaaS capabilities and show that window based state monitoring is not only more resilient to noises and outliers, but also saves considerable communication cost. Similarly, violation-likelihood based state monitoring can dynamically adjust monitoring intensity based on the likelihood of detecting important events, leading to significant gain in monitoring service consolidation. Finally, smart multi-tenancy support in state monitoring allows multiple cloud users to enjoy MaaS with improved performance and efficiency at more affordable cost. We perform extensive experiments in an emulated cloud environment with real world system and network traces. The experimental results suggest that our MaaS framework achieves significant lower monitoring cost, higher scalability and better multi-tenancy performance.},
 author = {Meng, Shicong and Liu, Ling},
 year = {2012},
 title = {Enhanced Monitoring-as-a-Service for Effective Cloud Management},
 keywords = {Communication/Networking and Information Technology;Computer Systems Organization;Computing Milieux;Distributed Systems;Management of Computing and Information Systems;Biomedical monitoring;Detection algorithms;Limiting;Monitoring;Scalability;Servers;Tuning},
 pages = {1--14},
 pagination = {page},
 journal = {IEEE Transactions on Computers},
 doi = {10.1109/TC.2012.165}
}


@article{Messerschmidt.2012,
 abstract = {Grid computing can meet computational demands and offers a promising resource utilization approach. However, little research details the drivers of and obstacles to adoption of this technology. Institutional and organizational capability theory suggests an adoption model that accounts for inter- and intra-organizational influences. An empirical study with 233 high-ranking IT executives reveals that adoption results from social contagion, while organizational capabilities such as trust, firm innovativeness, tendency to outsource, and IT department size, influence adoption from an intra-organizational perspective. The findings show that mimetic pressures and trust play major roles in adoption processes, which differentiates grid computing from other inter-organizational systems.},
 author = {Messerschmidt, Christian M. and Hinz, Oliver},
 year = {2012},
 title = {Explaining the Adoption of Grid Computing: An Integrated Institutional Theory and Organizational Capability Approach},
 pages = {1--20},
 pagination = {page},
 journal = {The Journal of Strategic Information Systems}
}


@article{Nallur.2012,
 abstract = {Cloud computing, with its promise of (almost) unlimited computation, storage and bandwidth, is increasingly becoming the infrastructure of choice for many organizations. As cloud offerings mature, service-based applications need to dynamically recompose themselves, to self-adapt to changing QoS requirements. In this paper, we present a decentralized mechanism for such self-adaptation, using market-based heuristics. We use a continuous double-auction to allow applications to decide which services to choose, amongst the many on offer. We view an application as a multi-agent system, and the cloud as a marketplace where many such applications self-adapt. We show through a simulation study that our mechanism is effective, for the individual application as well as from the collective perspective of all applications adapting at the same time.},
 author = {Nallur, Vivek and Bahsoon, Rami},
 year = {2012},
 title = {A Decentralized Self-Adaptation Mechanism for Service-Based Applications in the Cloud},
 pages = {1--181},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Software Engineering},
 doi = {10.1109/TSE.2012.53}
}


@article{Oberle.2013,
 abstract = {Through the rise of cloud computing, on-demand applications, and business networks, services are increasingly being exposed and delivered on the Internet and through mobile communications. So far, services have mainly been described through technical interface descriptions. The description of business details, such as pricing, service-level, or licensing, has been neglected and is therefore hard to automatically process by service consumers. Also, third-party intermediaries, such as brokers, cloud providers, or channel partners, are interested in the business details in order to extend services and their delivery and, thus, further monetize services. In this paper, the constructivist design of the Unified Service Description Language (USDL), aimed at describing services across the human-to-automation continuum, is presented. The proposal of USDL follows well-defined requirements which are expressed against a common service discourse and synthesized from currently available service description efforts. USDL's concepts and modules are evaluated for their support of the different requirements and use cases.},
 author = {Oberle, Daniel and Barros, Alistair and Kylau, Uwe and Heinzl, Steffen},
 year = {2013},
 title = {A Unified Description Language for Human to Automated Services},
 pages = {155--181},
 pagination = {page},
 volume = {38},
 number = {1},
 journal = {Information Systems}
}


@article{Papagianni.2013,
 abstract = {Cloud computing builds upon advances on virtualization and distributed computing to support cost-efficient usage of computing resources, emphasizing on resource scalability and on demand services. Moving away from traditional data-center oriented models, distributed clouds extend over a loosely coupled federated substrate, offering enhanced communication and computational services to target end-users with Quality of Service (QoS) requirements, as dictated by the Future Internet vision. Towards facilitating the efficient realization of such networked computing environments, computing and networking resources need to be jointly treated and optimized. In this paper, towards providing a unified resource allocation framework for networked clouds, we first formulate the optimal networked cloud mapping problem as a mixed integer programming (MIP) problem, indicating objectives related to cost-efficiency of the resource mapping procedure, while abiding by user requests for QoS-aware virtual resources. We subsequently propose a method for the efficient mapping of resource requests onto a shared substrate interconnecting various islands of computing resources, and adopt a heuristic methodology to address the problem. The efficiency of the propoed approach is illustrated in a simulation/emulation environment, that allows for a flexible, structured and comparative performance evaluation. We conclude by outlining a proof-of-concept realization of our proposed schema, over FI test-bed FEDERICA, a resource virtualization platform augmented with network and computing facilities.},
 author = {Papagianni, Chrysa and Leivadeas, Aris and Papavassiliou, Symeon and Maglaris, Vassilis and Cervello-Pastor, Cristina and Monje, Alvaro},
 year = {2013},
 title = {On the Optimal Allocation of Virtual Resources in Cloud Computing Networks},
 pages = {1--13},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Computers}
}


@article{Patel.2012,
 abstract = {Due to prohibitive cost of datacenter setup and maintenance, many small-scale businesses rely on hosting centers to provide the cloud infrastructure to run their workloads. Hosting centers host services of the clients on their behalf and guarantee quality of service as defined by service level agreements (SLAs.) To reduce energy consumption and to maximize profit it is critical to optimally allocate resources to meet client SLAs. Optimal allocation is a non-trivial task due to 1) resource heterogeneity where energy consumption of a client task varies depending on the allocated resources 2) lack of energy proportionality where energy cost for a task varies based on server utilization. In this paper we introduce a generalized Network Flow based Resource Allocation framework, called NFRA, for energy minimization and profit maximization. NFRA provides a unified framework to model profit maximization under a wide range of SLAs. We will demonstrate the simplicity of this unified framework by deriving optimal resource allocations for three different SLAs. We derive workload demands and server energy consumption data from SPECWeb2009 benchmark results to demonstrate the efficiency of NFRA framework.},
 author = {Patel, Kimish and Annavaram, Murali and Pedram, Massoud},
 year = {2012},
 title = {NFRA: Generalized Network Flow Based Resource Allocation for Hosting Centers},
 pages = {1--14},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Computers}
}


@article{Praeg.2006,
 author = {Praeg, Claus-Peter and Schnabel, Ulrich},
 year = {2006},
 title = {IT-Service Cachet - Managing IT-Service Performance and IT-Service Quality},
 pages = {1--10},
 pagination = {page},
 journal = {Proceedings of the 39th Hawaii International Conference on Systems Sciences}
}


@article{Pueschel.2012,
 abstract = {Successful Internet service offerings can only thrive if customers are satisfied with service performance. While large service providers can usually cope with fluctuations of customer visits retaining acceptable Quality of Service, small and medium-sizes enterprises face a big challenge due to limited resources in the IT infrastructure. Popular services, such as justin.tv and SmugMug, rely on external resources provided by cloud computing providers in order to satisfy their customers demands at all times. The paradigm of cloud computing refers to the delivery model of computing services as a utility in a pay-as-you-go manner. In this paper, we provide and computationally evaluate decision models and policies that can help cloud computing providers increase their revenue under the realistic assumption of scarce resources and under both informational certainty and uncertainty of customers? resource requirement predictions. Our results show that in both cases under certainty and under uncertainty applying the dynamic pricing policy significantly increases revenue while using the client classification policy substantially reduces revenue. We also show that, for all policies, the presence of uncertainty causes losses in revenue; when the client classification policy is applied, losses can even amount to more than 8{\%}.},
 author = {P{\"u}schel, Tim and Schryen, Guido and Hristova, Diana and Neumann, Dirk},
 year = {2012},
 title = {Cloud Service Revenue Management},
 pages = {1--13},
 pagination = {page},
 journal = {European Conference on Information Systems}
}


@article{Qi.2012,
 abstract = {With the increasing popularity of~cloud~computing technologies, more and more~service~composition processes are enacted and executed in could environment. Compared with the various and approximately infinite application requirements from end users, the web services held by a~cloudplatform~are usually limited. Therefore, it is often a challenging effort to develop a~service~composition, in such a situation that only part of the functional qualified candidate services could be found inside a~cloud~platform. In this situation, the absent services will be invocated in a cross-platform~way outside the~cloud~platform. In view of this challenge, a QoS-aware composition method is investigated for supporting cross-platform~service~invocation in~cloudenvironment. Furthermore, some experiments are deployed to evaluate the method presented in this paper},
 author = {Qi, Lianyong and Dou, Wanchun and Zhang, Xuyun and Chen, Jinjun},
 year = {2012},
 title = {A QoS-Aware Composition Method Supporting Cross-Platform Service Invocation in Cloud Environment},
 pages = {1316--1329},
 pagination = {page},
 volume = {78},
 number = {5},
 journal = {Journal of Computer {\&} System Sciences}
}


@article{Repschlaeger.2012,
 abstract = {Due to the fast growth, Cloud Computing has become a non-transparent market with providers and customers willing to adopt it. Furthermore, many offers only partially meet customers? requirements and it is not clear how exactly Cloud Computing influences the IT. That makes it difficult for customers to plan migration projects and implement sustainable Cloud solutions. There are important factors and considerations for the decision to adopt Cloud Computing. The current studies and research in this field can be summarized to focus around the questions why adoption of Cloud Computing would occur, how much adoption would take place or how it would be adopted. But the adoption requirements covering all three service models (SaaS, PaaS, IaaS) have barely been discussed in literature so far. A detailed understanding of Cloud requirements enables customers to adopt Cloud solutions efficiently. Therefore this paper aims to contribute a framework addressing the adoption and selection of Cloud services. A Cloud Requirement Framework (CRF) was developed, concentrating on relevant requirements for adopting Cloud services targeting all three service models. To develop this framework we followed a design science approach and conducted a systematic literature review, an extensive market analysis and an evaluation based on expert interviews.},
 author = {Repschlaeger, Jonas and Zarnekow, Ruediger and Wind, Stefan and Turowski, Klaus},
 year = {2012},
 title = {Cloud Requirement Framework: Requirements and Evaluation Criteria to Adopt Cloud Solutions},
 pages = {1--14},
 pagination = {page},
 journal = {European Conference on Information Systems}
}


@article{Retana.2012,
 abstract = {Using a unique data set on public cloud infrastructure services consumption by 15,076 firms over the period from March 2009 to October 2011, we address the question of how a provider's technical support influences cloud demand. The provider's customers can choose (and switch between) two levels of support, basic and managed, which differ in the extent to which the provider helps customers adapt the cloud infrastructure to their specific business needs. We find that customers who access managed support consume, on average, 110{\%} more IT capacity than those who only access basic support. The former are also 15.5 percentage points more likely to deploy more complex infrastructure architectures that make better use of the cloud's features (e.g., its scalability). Customers who switch from managed to basic support continue consuming an average of 90{\%} more IT capacity than customers who only access basic support.},
 author = {Retana, German and Forman, Chris and Narasimhan, Sridhar and Niculescu, Marius F. and Wu, D. J.},
 year = {2012},
 title = {Technical Support and IT Capacity Demand: Evidence from the Cloud},
 pages = {1--7},
 pagination = {page},
 journal = {International Conference on Information Systems}
}


@misc{Saunders.2005,
 author = {Saunders, Cicely},
 year = {2005},
 title = {MIS Journal Rankings},
 url = {http://ais.affiniscape.com/displaycommon.cfm?an=1\&subarticlenbr=432},
 urldate = {2013-03-30}
}


@article{Schneider.2013,
 author = {Schneider, Stephan and Sunyaev, Ali},
 year = {2013},
 title = {Addressing Challenges During a Cloud Service's Life Cycle: Comparing Insights from Research and Practice},
 pages = {1--26},
 pagination = {page},
 journal = {Working Paper Series, University of Cologne}
}


@article{Schroedl.2012,
 abstract = {The implementation of electronic procurement processes for product-service systems, consisting of material and service components requires a consideration of strategic, tactical and operational issues in procurement processes and information technologies. Increasingly, in certain industries, these product-service systems consist more and more of cloud-based components like online storage or web applications. In the past, the alignment of business processes with a focus on traditional procurement processes for products or services has been well established. But with the rise of product-service systems as core offering from companies, the design of hybrid procurement processes in value networks has to be developed. The merging of different procurement processes for products and services, however, has severe problems and does not reflect the specific requirements in the procurement of product-service systems, especially with cloud-based components. This article highlights the need for a process-oriented view in procurement at multiple levels of abstraction and describes a model for the design of electronic procurement process in value networks for cloud-based product-service systems requirements. Different process characteristics are examined for applicability to hybrid value performance and allow an adjustment proposal for the hybrid procurement process. The proposed procurement model is validated in a typical case-study in the IT industry.},
 author = {Schr{\"o}dl, Holger},
 year = {2012},
 title = {Purchasing Cloud-Based Product-Service Bundles in Value Networks - The Role of Manageable Workloads},
 pages = {1--13},
 pagination = {page},
 journal = {European Conference on Information Systems}
}


@article{Silic.2013,
 abstract = {The modern information systems on the Internet are often implemented as composite services built from multiple atomic services. These atomic services have their interfaces publicly available while their inner structure is unknown. The quality of the composite service is dependent on both the availability of each atomic service and their appropriate orchestration. In this paper, we present LUCS, a formal model for predicting the availability of atomic web services that enhances the current state-of-the-art models used in service recommendation systems. LUCS estimates the service availability for an ongoing request by considering its similarity to prior requests according to the following dimensions: the user's and service's geographic location, the service load, and the service's computational requirements. In order to evaluate our model, we conducted experiments on services deployed in different regions of the Amazon cloud. For each service, we varied the geographic origin of its incoming requests as well as the request frequency. The evaluation results suggest that our model significantly improves availability prediction when all of the LUCS input parameters are available, reducing the prediction error by 71{\%} compared to the current state-of-the-art.},
 author = {Silic, Marin and Delac, Goran and Krka, Ivo and Srbljic, Sinisa},
 year = {2013},
 title = {Scalable and Accurate Prediction of Availability of Atomic Web Services},
 pages = {1--14},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Services Computing}
}


@article{Sim.2012,
 abstract = {Agent-based cloud computing is concerned with the design and development of software agents for bolstering cloud service discovery, service negotiation, and service composition. The significance of this work is introducing an agent-based paradigm for constructing software tools and testbeds for cloud resource management. The novel contributions of this work include: 1) developing Cloudle: an agent-based search engine for cloud service discovery, 2) showing that agent-based negotiation mechanisms can be effectively adopted for bolstering cloud service negotiation and cloud commerce, and 3) showing that agent-based cooperative problem-solving techniques can be effectively adopted for automating cloud service composition. Cloudle consists of 1) a service discovery agent that consults a cloud ontology for determining the similarities between providers' service specifications and consumers' service requirements, and 2) multiple cloud crawlers for building its database of services. Cloudle supports three types of reasoning: similarity reasoning, compatibility reasoning, and numerical reasoning. To support cloud commerce, this work devised a complex cloud negotiation mechanism that supports parallel negotiation activities in interrelated markets: a cloud service market between consumer agents and broker agents, and multiple cloud resource markets between broker agents and provider agents. Empirical results show that using the complex cloud negotiation mechanism, agents achieved high utilities and high success rates in negotiating for cloud resources. To automate cloud service composition, agents in this work adopt a focused selection contract net protocol (FSCNP) for dynamically selecting cloud services and use service capability tables (SCTs) to record the list of cloud agents and their services. Empirical results show that using FSCNP and SCTs, agents can successfully compose cloud services by autonomously selecting services.},
 author = {Sim, Kwang Mong},
 year = {2012},
 title = {Agent-Based Cloud Computing},
 pages = {564--577},
 pagination = {page},
 volume = {5},
 number = {4},
 journal = {IEEE Transactions on Services Computing}
}


@book{Sommerville.2012,
 author = {Sommerville, Ian},
 year = {2012},
 title = {Software Engineering},
 keywords = {Software Engineering},
 address = {M{\"u}nchen},
 edition = {9},
 publisher = {Pearson},
 isbn = {9783868940992},
 series = {Pearson Studium - IT}
}


@article{Son.2012,
 abstract = {When making reservations for Cloud services, consumers and providers need to establish service-level agreements through negotiation. Whereas it is essential for both a consumer and a provider to reach an agreement on the price of a service and when to use the service, to date, there is little or no negotiation support for both price and time-slot negotiations (PTNs) for Cloud service reservations. This paper presents a multi-issue negotiation mechanism to facilitate the following: 1) PTNs between Cloud agents and 2) tradeoff between price and time-slot utilities. Unlike many existing negotiation mechanisms in which a negotiation agent can only make one proposal at a time, agents in this work are designed to concurrently make multiple proposals in a negotiation round that generate the same aggregated utility, differing only in terms of individual price and time-slot utilities. Another novelty of this work is formulating a novel time-slot utility function that characterizes preferences for different time slots. These ideas are implemented in an agent-based Cloud testbed. Using the testbed, experiments were carried out to compare this work with related approaches. Empirical results show that PTN agents reach faster agreements and achieve higher utilities than other related approaches. A case study was carried out to demonstrate the application of the PTN mechanism for pricing Cloud resources.},
 author = {Son, Seokho and Sim, Kwang-Mong},
 year = {2012},
 title = {A Price- and-Time-Slot-Negotiation Mechanism for Cloud Service Reservations},
 pages = {713--728},
 pagination = {page},
 volume = {42},
 number = {3},
 journal = {IEEE Transactions on Systems, Man {\&} Cybernetics: Part B}
}


@article{Suciu.2012,
 abstract = {Cloud services are very popular today. One can rent platforms, software, or applications from companies like Amazon, Google, Microsoft, or Salesforce. Whenever users rent their services, they trust these cloud companies with their confidential data, ranging from benign personal email messages and pictures to highly sensitive financial data or medical records. Currently, the only means for cloud companies to earn users' trust is to have very strict internal policies for managing and restricting access to users' data, and to use conventional system security to resist hackers and external adversaries. Commercial database systems like DB2, Oracle, and SQL Server allow the database to be encrypted, but either require the application to be rewritten, or require the database server to have access to the encryption key.},
 author = {Suciu, Dan},
 year = {2012},
 title = {Technical Perspective: SQL on an Encrypted Database},
 pages = {102},
 pagination = {page},
 journal = {Association for Computing Machinery. Communications of the ACM}
}


@article{Sundareswaran.2012,
 abstract = {Cloud computing enables highly scalable services to be easily consumed over the Internet on an as-needed basis. A major feature of the cloud services is that users' data are usually processed remotely in unknown machines that users do not own or operate. While enjoying the convenience brought by this new emerging technology, users' fears of losing control of their own data (particularly, financial and health data) can become a significant barrier to the wide adoption of cloud services. To address this problem, in this paper, we propose a novel highly decentralized information accountability framework to keep track of the actual usage of the users' data in the cloud. In particular, we propose an object-centered approach that enables enclosing our logging mechanism together with users' data and policies. We leverage the JAR programmable capabilities to both create a dynamic and traveling object, and to ensure that any access to users' data will trigger authentication and automated logging local to the JARs. To strengthen user's control, we also provide distributed auditing mechanisms. We provide extensive experimental studies that demonstrate the efficiency and effectiveness of the proposed approaches.},
 author = {Sundareswaran, Smitha and Squicciarini, Anna C. and Lin, Dan},
 year = {2012},
 title = {Ensuring Distributed Accountability for Data Sharing in the Cloud},
 pages = {556--568},
 pagination = {page},
 volume = {9},
 number = {4},
 journal = {IEEE Transactions on Dependable and Secure Computing}
}


@article{Tang.2012,
 abstract = {We can now outsource data backups off-site to third-party cloud storage services so as to reduce data management costs. However, we must provide security guarantees for the outsourced data, which is now maintained by third parties. We design and implement FADE, a secure overlay cloud storage system that achieves fine-grained, policy-based access control and file assured deletion. It associates outsourced files with file access policies, and assuredly deletes files to make them unrecoverable to anyone upon revocations of file access policies. To achieve such security goals, FADE is built upon a set of cryptographic key operations that are self-maintained by a quorum of key managers that are independent of third-party clouds. In particular, FADE acts as an overlay system that works seamlessly atop today's cloud storage services. We implement a proof-of-concept prototype of FADE atop Amazon S3, one of today's cloud storage services. We conduct extensive empirical studies, and demonstrate that FADE provides security protection for outsourced data, while introducing only minimal performance and monetary cost overhead. Our work provides insights of how to incorporate value-added security features into today's cloud storage services.},
 author = {Tang, Yang and Lee, Patrick P. C. and Lui, John C. S. and Perlman, Radia},
 year = {2012},
 title = {Secure Overlay Cloud Storage with Access Control and Assured Deletion},
 pages = {903--916},
 pagination = {page},
 journal = {IEEE Transactions on Dependable and Secure Computing}
}


@book{Taylor.2007,
 abstract = {A view of ITIL that aligns business and IT so that each brings out the best in the other. It ensures that every element of the service lifecycle is focused on customer outcomes and relates to all the companion process elements that follow. Subsequent titles in the core set will link deliverables to meeting the business goals, requirements and service management principles described in this publication.},
 author = {Taylor, Sharon and Iqbal, Majid and Nieves, Michael},
 year = {2007},
 title = {Service Strategy},
 keywords = {Computer industry;Electronic office machine industry;Business planning;IT infrastructure library},
 address = {London},
 publisher = {The Stationery Office},
 isbn = {0113310455}
}


@article{TolosanaCalasanz.2012,
 abstract = {The ability to support Quality of~Service~(QoS) constraints is an important requirement in some scientific applications. With the increasing use of~Cloud~computing infrastructures, where access to resources is shared, dynamic and provisioned on-demand, identifying how QoS constraints can be supported becomes an important challenge. However, access to dedicated resources is often not possible in existing~Cloud~deployments and limited QoS guarantees are provided by many commercial providers (often restricted to error rate and availability, rather than particular QoS metrics such as latency or access time). We propose a workflow system architecture which enforces QoS for the simultaneous execution of multiple scientific workflows over a shared~infrastructure~(such as a~Cloud~environment). Our approach involves multiple pipeline workflow instances, with each instance having its own QoS requirements. These workflows are composed of a number of stages, with each stage being mapped to one or more physical resources. A stage involves a combination of data access, computation and data transfer capability. A token bucket-based data throttling framework is embedded into the workflow system architecture. Each workflow instance stage regulates the amount of data that is injected into the shared resources, allowing for bursts of data to be injected while at the same time providing isolation of workflow streams. We demonstrate our approach by using the Montage workflow, and develop a Reference net model of the workflow.},
 author = {Tolosana-Calasanz, Rafael and Ba{\~n}ares, Jos{\'e} {\'A}. and Pham, Congduc and Rana, Omer F.},
 year = {2012},
 title = {Enforcing QoS in Scientific Workflow Systems Enacted over Cloud Infrastructures},
 pages = {1300--1315},
 pagination = {page},
 volume = {78},
 number = {5},
 journal = {Journal of Computer {\&} System Sciences}
}


@article{Villegas.2012,
 abstract = {We show how a layered Cloud service model of software (SaaS), platform (PaaS), and infrastructure (IaaS) leverages multiple independent Clouds by creating a federation among the providers. The layered architecture leads naturally to a design in which inter-Cloud federation takes place at each service layer, mediated by a broker specific to the concerns of the parties at that layer. Federation increases consumer value for and facilitates providing IT services as a commodity. This business model for the Cloud is consistent with broker mediated supply and service delivery chains in other commodity sectors such as finance and manufacturing. Concreteness is added to the federated Cloud model by considering how it works in delivering the Weather Research and Forecasting service (WRF) as SaaS using PaaS and IaaS support. WRF is used to illustrate the concepts of delegation and federation, the translation of service requirements between service layers, and inter-Cloud broker functions needed to achieve federation.},
 author = {Villegas, David and Bobroff, Norman and Rodero, Ivan and Delgado, Javier and Liu, Yanbin and Devarakonda, Aditya and Fong, Liana and Masoud Sadjadi, S. and Parashar, Manish},
 year = {2012},
 title = {Cloud Federation in a Layered Service Model},
 pages = {1330--1344},
 pagination = {page},
 volume = {78},
 number = {5},
 journal = {Journal of Computer {\&} System Sciences}
}


@book{Voecking.2008,
 year = {2008},
 title = {Taschenbuch der Algorithmen},
 address = {Berlin and Heidelberg},
 publisher = {Springer},
 isbn = {978-3-540-76393-2},
 series = {EXamen.press},
 editor = {V{\"o}cking, Berthold}
}


@article{Wang.2012,
 abstract = {Cloud storage enables users to remotely store their data and enjoy the on-demand high quality cloud applications without the burden of local hardware and software management. Though the benefits are clear, such a service is also relinquishing users' physical possession of their outsourced data, which inevitably poses new security risks toward the correctness of the data in cloud. In order to address this new problem and further achieve a secure and dependable cloud storage service, we propose in this paper a flexible distributed storage integrity auditing mechanism, utilizing the homomorphic token and distributed erasure-coded data. The proposed design allows users to audit the cloud storage with very lightweight communication and computation cost. The auditing result not only ensures strong cloud storage correctness guarantee, but also simultaneously achieves fast data error localization, i.e., the identification of misbehaving server. Considering the cloud data are dynamic in nature, the proposed design further supports secure and efficient dynamic operations on outsourced data, including block modification, deletion, and append. Analysis shows the proposed scheme is highly efficient and resilient against Byzantine failure, malicious data modification attack, and even server colluding attacks.},
 author = {Wang, Cong and Wang, Qian and Ren, Kui and Cao, Ning and Lou, Wenjing},
 year = {2012},
 title = {Toward Secure and Dependable Storage Services in Cloud Computing},
 pages = {220--232},
 pagination = {page},
 volume = {5},
 number = {2},
 journal = {IEEE Transactions on Services Computing}
}


@article{Wang.2012b,
 abstract = {Recently, cloud computing rapidly expands as an alternative to conventional computing due to it can provide a flexible, dynamic and resilient infrastructure for both academic and business environments. In public cloud environment, the client moves its data to public cloud server (PCS) and can not control its remote data. Thus, information security is an important problem in public cloud storage, such as data confidentiality, integrity, and availability. In some cases, the client has no ability to check its remote data possession, such as the client is in prison because of comitting crime, on the ocean-going vessel, in the battlefield because of the war, {\{}em et al{\}}. It has to delegate the remote data possession checking task to some proxy. In this paper, we study proxy provable data possession (PPDP). In public clouds, PPDP is a matter of crucial importance when the client can not perform the remote data possession checking. We study the PPDP system model, security model and design method. Based on the bilinear pairing technique, we design an efficient PPDP protocol. Through security analysis and performance analysis, our protocol is provable secure and efficient.},
 author = {Wang, Huaqun},
 year = {2012},
 title = {Proxy Provable Data Possession in Public Clouds},
 pages = {1--9},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Services Computing}
}


@article{Wang.2013,
 abstract = {Using cloud storage, users can remotely store their data and enjoy the on-demand high-quality applications and services from a shared pool of configurable computing resources, without the burden of local data storage and maintenance. However, the fact that users no longer have physical possession of the outsourced data makes the data integrity protection in cloud computing a formidable task, especially for users with constrained computing resources. Moreover, users should be able to just use the cloud storage as if it is local, without worrying about the need to verify its integrity. Thus, enabling public auditability for cloud storage is of critical importance so that users can resort to a third-party auditor (TPA) to check the integrity of outsourced data and be worry free. To securely introduce an effective TPA, the auditing process should bring in no new vulnerabilities toward user data privacy, and introduce no additional online burden to user. In this paper, we propose a secure cloud storage system supporting privacy-preserving public auditing. We further extend our result to enable the TPA to perform audits for multiple users simultaneously and efficiently. Extensive security and performance analysis show the proposed schemes are provably secure and highly efficient. Our preliminary experiment conducted on Amazon EC2 instance further demonstrates the fast performance of the design.},
 author = {Wang, Cong and Chow, Sherman S. M. and Wang, Qian and Ren, Kui and Lou, Wenjing},
 year = {2013},
 title = {Privacy-Preserving Public Auditing for Secure Cloud Storage},
 pages = {362--375},
 pagination = {page},
 volume = {62},
 number = {2},
 journal = {IEEE Transactions on Computers}
}


@article{Webster.2002,
 abstract = {A review of prior, relevant literature is an essential feature of any academic project. An effective review creates a firm foundation for advancing knowledge. It facilitates theory development, closes areas where a plethora of research exists, and uncovers areas where research is needed.},
 author = {Webster, Jane and Watson, Richard T.},
 year = {2002},
 title = {Analyzing the Past to Prepare for the Future: Writing a Literature Review},
 pages = {xiii--xxiii},
 pagination = {page},
 volume = {26},
 number = {2},
 journal = {MIS Quarterly}
}


@article{Wei.2012,
 abstract = {NoSQL cloud data stores provide scalability and high availability properties for web applications, but at the same time they sacrifice data consistency. However, many applications cannot afford any data inconsistency. CloudTPS is a scalable transaction manager which guarantees full ACID properties for multi-item transactions issued by web applications, even in the presence of server failures and network partitions. We implement this approach on top of the two main families of scalable data layers: Bigtable and SimpleDB. Performance evaluation on top of HBase (an open-source version of Bigtable) in our local cluster and Amazon SimpleDB in the Amazon cloud shows that our system scales linearly at least up to 40 nodes in our local cluster and 80 nodes in the Amazon cloud.},
 author = {Wei, Zhou and Pierre, Guillaume and Chi, Chi-Hung},
 year = {2012},
 title = {CloudTPS: Scalable Transactions for Web Applications in the Cloud},
 pages = {525--539},
 pagination = {page},
 volume = {5},
 number = {4},
 journal = {IEEE Transactions on Services Computing}
}


@article{Weinhardt.2009,
 abstract = {In letzter Zeit hat ein neues Rechen- und Architekturparadigma Einzug gehalten, das sogenannte Cloud-Computing. Es wird mindestens so umfangreich beworben wie Grid- Computing wenige Jahre zuvor, was u. a. zahlreiche Diskussionen {\"u}ber die Abgrenzung zwischen Grid- und Cloud-Computing angeregt hat. Der erste Beitrag dieses Artikels ist deshalb die eingehende Diskussion der verschiedenen Charakteristika von jeweils Grid- und Cloud-Computing. Diese technische Abgrenzung erm{\"o}glicht eine fundierte Diskussion der Gesch{\"a}ftschancen des Cloud-Computing-Paradigmas. In diesem Arti- kel wird hierzu zun{\"a}chst ein Framework f{\"u}r Gesch{\"a}ftsmodelle in Clouds aufgestellt. Anschlie{\ss}end werden aktuelle Cloud-Angebote besprochen und in das Framework eingereiht. Zum Schluss spricht dieser Artikel Herausforderungen und erfolgverspre- chende Forschungsgebiete an, die adressiert und bew{\"a}ltigt werden m{\"u}ssen, damit die Vision von Clouds erfolgreich umgesetzt werden kann.},
 author = {Weinhardt, Christof and Anandasivam, Arun and Blau, Benjamin and Borissov, Nikolay and Meinl, Thomas and Michalk, Wiebke and St{\"o}{\ss}er, Jochen},
 year = {2009},
 title = {Cloud-Computing - Eine Abgrenzung, Geschäftsmodelle und Forschungsgebiete},
 keywords = {Cloud-Computing;Grid-Computing;Gesch{\"a}ftsmodelle;Forschungsgebiete;Cloud Computing;Grid computing;Business models;Research directions},
 pages = {453--462},
 pagination = {page},
 volume = {51},
 number = {1},
 journal = {WIRTSCHAFTSINFORMATIK}
}


@book{Wessels.2001,
 author = {Wessels, Duane},
 year = {2001},
 title = {Web Caching},
 address = {Sebastopol and CA},
 edition = {1},
 publisher = {O'Reilly {\&} Associates},
 isbn = {1-56592-536-x}
}


@article{Wu.2012,
 abstract = {Software as a Service (SaaS) provides access to applications to},
 author = {Wu, Linlin and Kumar Garg, Saurabh and Buyya, Rajkumar},
 year = {2012},
 title = {SLA-Based Admission Control for a Software-as-a-Service Provider},
 pages = {1--16},
 pagination = {page},
 volume = {78},
 number = {5},
 journal = {Journal of Computer {\&} System Sciences}
}


@article{Xiao.2012,
 abstract = {Many Internet applications can benefit from an automatic scaling property where their resource usage can be scaled up and down automatically by the cloud service provider. We present a system that provides automatic scaling for Internet applications in the cloud environment. We encapsulate each application instance inside a virtual machine (VM) and use virtualization technology to provide fault isolation. We model it as the Class Constrained Bin Packing (CCBP) problem where each server is a bin and each class represents an application. The class constraint reflects the practical limit on the number of applications a server can run simultaneously. We develop an efficient semi-online color set algorithm that achieves good demand satisfaction ratio and saves energy by reducing the number of servers used when the load is low. Experiment results demonstrate that our system can improve the throughput by 180{\%} over an open source implementation of Amazon EC2 and restore the normal QoS five times as fast during flash crowds. Large scale simulations demonstrate that our algorithm is extremely scalable: the decision time remains under 4 seconds for a system with 10,000 servers and 10,000 applications. This is an order of magnitude improvement over traditional application placement algorithms in enterprise environments.},
 author = {Xiao, Zhen and Chen, Qi and Luo, Haipeng},
 year = {2012},
 title = {Automatic Scaling of Internet Applications for Cloud Computing Services},
 keywords = {Design Tools and Techniques;Distributed Systems;Distributed/Internet based software engineering tools and techniques;Operating Systems;Organization and Design;Software Engineering;Software/Software Engineering},
 pages = {1--14},
 pagination = {page},
 volume = {10},
 number = {10},
 journal = {IEEE Transactions on Computers},
 doi = {10.1109/TC.2012.284}
}


@article{Xu.2013,
 abstract = {In modern parallel storage systems (e.g., cloud storage and data centers), it is important to provide data availability guarantees against disk (or storage node) failures via redundancy coding schemes. One coding scheme is X-code, which is double-fault tolerant while achieving the optimal update complexity. When a disk/node fails, recovery must be carried out to reduce the possibility of data unavailability. We propose an X-code-based optimal recovery scheme called Minimum-Disk-Read-Recovery (MDRR), which minimizes the number of disk reads for single-disk failure recovery. We make several contributions. First, we show that MDRR provides optimal single-disk failure recovery and reduces about 25{\%} of disk reads compared to the conventional recovery approach. Second, we prove that any optimal recovery scheme for X-code cannot balance disk reads among different disks within a single stripe in general cases. Third, we propose an efficient logical encoding scheme that issues balanced disk read in a group of stripes for any recovery algorithm (including the MDRR scheme). Finally, we implement our proposed recovery schemes and conduct extensive testbed experiments in a networked storage system prototype. Experiments indicate that MDRR reduces around 20{\%} of recovery time of the conventional approach, showing that our theoretical findings are applicable in practice.},
 author = {Xu, Silei and Li, Runhui and Lee, Patrick P. C. and Zhu, Yunfeng and Xiang, Liping and Xu, Yinlong and Lui, John C. S.},
 year = {2013},
 title = {Single Disk Failure Recovery for X-Code-Based Parallel Storage Systems},
 pages = {1--14},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Computers}
}


@article{Yu.2013,
 abstract = {Cloud computing has emerging as a promising pattern for data outsourcing and high-quality data services. However, concerns of sensitive information on cloud potentially causes privacy problems. Data encryption protects data security to some extent, but at the cost of compromised efficiency. Searchable symmetric encryption (SSE) allows retrieval of encrypted data over cloud. In this paper, we focus on addressing data privacy issues using searchable symmetric encryption (SSE). For the first time, we formulate the privacy issue from the aspect of similarity relevance and scheme robustness. We observe that server-side ranking based on order-preserving encryption (OPE) inevitably leaks data privacy. To eliminate the leakage, we propose a two-round searchable encryption (TRSE) scheme that supports top-k multi-keyword retrieval. In TRSE, we employ a vector space model and homomorphic encryption. The vector space model helps to provide sufficient search accuracy, and the homomorphic encryption enables users to involve in the ranking while the majority of computing work is done on the server side by operations only on ciphertext. As a result, information leakage can be eliminated and data security is ensured. Thorough security and performance analysis show that the proposed scheme guarantees high security and practical efficiency.},
 author = {Yu, Jiadi and Lu, Peng and Zhu, Yanmin and Xue, Guangtao and Li, Minglu},
 year = {2013},
 title = {Towards Secure Multi-Keyword Top-k Retrieval over Encrypted Cloud Data},
 pages = {1--30},
 pagination = {page},
 number = {99},
 journal = {IEEE Transactions on Dependable and Secure Computing}
}


@article{Zhan.2012,
 abstract = {Recent cost analysis shows that the server cost still dominates the total cost of high-scale data centers or cloud systems. In this paper, we argue for a new twist on the classical resource provisioning problem: heterogeneous workloads are a fact of life in large-scale data centers, and current resource provisioning solutions do not act upon this heterogeneity. Our contributions are threefold: first, we propose a cooperative resource provisioning solution, and take advantage of differences of heterogeneous workloads so as to decrease their peak resources consumption under competitive conditions; second, for four typical heterogeneous workloadss, we build an agile system PhoenixCloud that enables cooperative resource provisioning; and third, we perform a comprehensive evaluation for both real and synthetic workload traces. Our experiments show that our solution could save the server cost aggressively with respect to the non-cooperative solutions that are widely used in state-of-the-practice hosting data centers or cloud systems. For a large amount of heterogeneous workloads, our solution could reduce resources consumption aggressively. For the maximum 120 synthetic workload traces, PhoenixCloud can save the peak resource consumption maximally by 66 percent and the total resource consumption maximally by 17 percent for the resource provider with respect to the non-cooperative solutions},
 author = {Zhan, Jianfeng and Wang, Lei and Li, Xiaona and Shi, Weisong and Weng, Chuliang and Zhang, Wenyao and Zang, Xiutao},
 year = {2012},
 title = {Cost-Aware Cooperative Resource Provisioning for\linebreak Heterogeneous Workloads in Data Centers},
 pages = {1--14},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Computers}
}


@article{Zhang.2012,
 abstract = {Cloud computing promises an open environment where customers can deploy IT services in pay-as-you-go fashion while saving huge capital investment in their own IT infrastructure. Due to the openness, various malicious service providers can exist. Such service providers may record service requests from a customer and then collectively deduce the customer private information. Therefore, customers need to take certain actions to protect their privacy. Obfuscation with noise injection, that mixes noise service requests with real customer service requests so that service providers will be confused about which requests are real ones, is an effective approach in this regard if those request occurrence probabilities are about the same. However, current obfuscation with noise injection uses random noise requests. Due to the randomness it needs a large number of noise requests to hide the real ones so that all of their occurrence probabilities are about the same, i.e. service providers would be confused. In pay-as-you-go cloud environment, a noise request will cost the same as a real request. Hence, with the same level of confusion, i.e. customer privacy protection, the number of noise requests should be kept as few as possible. Therefore in this paper we develop a novel historical probability based noise generation strategy. Our strategy generates noise requests based on their historical occurrence probability so that all requests including noise and real ones can reach about the same occurrence probability, and then service providers would not be able to distinguish in between. Our strategy can significantly reduce the number of noise requests over the random strategy, by more than 90{\%} as demonstrated by simulation evaluation.},
 author = {Zhang, Gaofeng and Yang, Yun and Chen, Jinjun},
 year = {2012},
 title = {A Historical Probability based Noise Generation Strategy for Privacy Protection in Cloud Computing},
 pages = {1374--1381},
 pagination = {page},
 volume = {78},
 number = {5},
 journal = {Journal of Computer {\&} System Sciences}
}


@article{Zhao.2013,
 abstract = {Currently, we are witnessing a proliferation in the number of cloud-hosted applications with a tremendous increase in the scale of the data generated as well as being consumed by such applications. The specifications of existing service level agreements (SLA) for cloud services are not designed to flexibly handle even relatively straightforward performance and technical requirements of consumer applications. In this article, we present a novel approach for SLA-based management of cloud-hosted databases from the consumer perspective. The framework facilitates adaptive and dynamic provisioning of the database tier of the software applications based on application-defined policies for satisfying their own SLA performance requirements, avoiding the cost of any SLA violation and controlling the monetary cost of the allocated computing resources. In this framework, the SLA of the consumer applications are declaratively defined in terms of goals which are subjected to a number of constraints that are specific to the application requirements. The framework continuously monitors the application-defined SLA and automatically triggers the execution of necessary corrective actions (scaling out/in the database tier) when required. The experimental results demonstrate the effectiveness of our SLA-based framework in providing the consumer applications with the required flexibility for achieving their SLA requirements.},
 author = {Zhao, Liang and Sakr, Sherif and Liu, Anna},
 year = {2013},
 title = {A Framework for Consumer-Centric SLA Management of Cloud-Hosted Databases},
 pages = {1},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Services Computing}
}


@article{Zheng.2012,
 abstract = {Cloud computing is becoming a mainstream aspect of information technology. More and more enterprises deploy their software systems in the cloud environment. The cloud applications are usually large scale and include a lot of distributed cloud components. Building highly reliable cloud applications is a challenging and critical research problem. To attack this challenge, we propose a component ranking framework, named FTCloud, for building fault-tolerant cloud applications. FTCloud includes two ranking algorithms. The first algorithm employs component invocation structures and invocation frequencies for making significant component ranking. The second ranking algorithm systematically fuses the system structure information as well as the application designers' wisdom to identify the significant components in a cloud application. After the component ranking phase, an algorithm is proposed to automatically determine an optimal fault-tolerance strategy for the significant cloud components. The experimental results show that by tolerating faults of a small part of the most significant components, the reliability of cloud applications can be greatly improved.},
 author = {Zheng, Zibin and Zhou, Tom Chao and Lyu, Michael R. and King, Irwin K.},
 year = {2012},
 title = {Component Ranking for Fault-Tolerant Cloud Applications},
 pages = {540--550},
 pagination = {page},
 volume = {5},
 number = {4},
 journal = {IEEE Transactions on Services Computing}
}


@article{Zhou.2012,
 abstract = {Advancements in cloud computing enable the easy deployment of numerous services. However, the analysis of cloud service access platforms from a client perspective shows that maintaining and managing clients remain a challenge for end-users. In this paper, we present the design, implementation, and evaluation of an asymmetric virtual machine monitor (AVMM), which is an asymmetric partitioning-based bare-metal approach that achieves near-native performance while supporting a new out-of-operating system mechanism for value-added services. To achieve these goals, AVMM divides underlying platforms into two asymmetric partitions: a user partition and a service partition. The user partition runs a commodity user OS, which is assigned to most of the underlying resources, maintaining end-user experience. The service partition runs a specialized OS, which consumes only the needed resources for its tasks and provides enhanced features to the user OS. AVMM considerably reduces virtualization overhead through two approaches: (1) Peripheral devices, such as graphics equipment, are assigned to be monopolized by a single user OS. (2) Efficient resource management mechanisms are leveraged to alleviate complicated resource sharing in existing virtualization technologies. We implement a prototype that supports Windows and Linux systems. Experimental results show that AVMM is a feasible and efficient approach to client virtualization.},
 author = {Zhou, Yuezhi and Zhang, Yaoxue and Liu, Hao and Xiong, Naixue and Vasilakos, Athanasios},
 year = {2012},
 title = {A Bare-Metal and Asymmetric Partitioning Approach to Client Virtualization},
 pages = {1--14},
 pagination = {page},
 volume = {PP},
 number = {99},
 journal = {IEEE Transactions on Services Computing}
}


@article{Zhu.2012,
 abstract = {The recent emergence of clouds is making the vision of utility computing realizable, i.e., computing resources and services can be delivered, utilized, and paid for as utilities such as water or electricity. This, however, creates new resource provisioning problems. Because of the pay-as-you-go model, resource provisioning should be performed in a way to keep resource costs to a minimum, while meeting an application's needs. In this work, we focus on the use of cloud resources for a class of adaptive applications, where there could be application-specific flexibility in the computation that may be desired. Furthermore, there may be a fixed time-limit as well as a resource budget. Within these constraints, such adaptive applications need to maximize their Quality of Service (QoS), more precisely, the value of an application-specific benefit function, by dynamically changing adaptive parameters. We present the design, implementation, and evaluation of a framework that can support such dynamic adaptation for applications in a cloud computing environment. The key component of our framework is a multi-input-multi-output feedback control model-based dynamic resource provisioning algorithm which adopts reinforcement learning to adjust adaptive parameters to guarantee the optimal application benefit within the time constraint. Then a trained resource model changes resource allocation accordingly to satisfy the budget. We have evaluated our framework with two real-world adaptive applications, and have demonstrated that our approach is effective and causes a very low overhead.},
 author = {Zhu, Qian and Agrawal, Gagan},
 year = {2012},
 title = {Resource Provisioning with Budget Constraints for Adaptive Applications in Cloud Environments},
 keywords = {Cloud Computing;adaptive applications;control theory;Adaptation models;Computational modeling;Dynamic scheduling;Heuristic algorithms;Pricing;Resource management;Time factors},
 pages = {497--511},
 pagination = {page},
 volume = {5},
 number = {4},
 journal = {IEEE Transactions on Services Computing},
 doi = {10.1109/TSC.2011.61}
}


@article{Zhu.2012b,
 abstract = {In this paper, we study the problem of reliable collective communication (broadcast or gossip) with the objective of maximizing the reliability of the collective communication. The need for collective communication arises in many problems of parallel and distributed computing, including Grid or cloud computing and database management. We describe the network model, formulate the reliable collective communication problem, prove that the maximum reliable collective communication problem is NP-hard, and provide an integer linear program (ILP) formulation for the problem. We then provide a greedy approximation algorithm to construct collective communication (through a spanning tree) that achieves an approximation ratio of 1 + ln(|V| + \textgreek{a}|E| - 1), where is the average number of shared link risk groups (SRLGs) along links, and |V| and |E| are the total number of vertices and edges of the network, respectively. Simulations demonstrate that our approximation algorithm achieves good performance in both small and large networks and that, in almost 95{\%} of total cases, our algorithm outperforms the modified minimum spanning tree algorithms.},
 author = {Zhu, Yi and Jue, Jason P.},
 year = {2012},
 title = {Reliable Collective Communications with Weighted SRLGs in Optical Networks},
 pages = {851--863},
 pagination = {page},
 volume = {20},
 number = {3},
 journal = {IEEE/ACM Transactions on Networking}
}


